<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>CS231n 2:线性分类器 | 炸牛奶超级甜</title><meta name="author" content="Jun Cao"><meta name="copyright" content="Jun Cao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="线性分类器 线性分类器简介 回顾Knn 图像分类的任务，就是从已有的固定分类标签集合中选择一个并分配给一张图像。cs231n中还介绍了k-Nearest Neighbor （k-NN）分类器，该分类器的基本思想是通过将测试图像与训练集带标签的图像进行比较，来给测试图像打上分类标签。k-Nearest Neighbor分类器存在以下不足：  分类器只是简单储存所有训练数据，然后和未来测试数据比较。没">
<meta property="og:type" content="article">
<meta property="og:title" content="CS231n 2:线性分类器">
<meta property="og:url" content="http://example.com/2022/10/29/CS231n/CS231n%203%EF%BC%9Asvm%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%99%A8/index.html">
<meta property="og:site_name" content="炸牛奶超级甜">
<meta property="og:description" content="线性分类器 线性分类器简介 回顾Knn 图像分类的任务，就是从已有的固定分类标签集合中选择一个并分配给一张图像。cs231n中还介绍了k-Nearest Neighbor （k-NN）分类器，该分类器的基本思想是通过将测试图像与训练集带标签的图像进行比较，来给测试图像打上分类标签。k-Nearest Neighbor分类器存在以下不足：  分类器只是简单储存所有训练数据，然后和未来测试数据比较。没">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i2.wp.com/dataaspirant.com/wp-content/uploads/2017/01/Support-vector-machine-svm.jpg?w=1024&ssl=1">
<meta property="article:published_time" content="2022-10-29T07:17:45.000Z">
<meta property="article:modified_time" content="2022-10-30T16:29:23.813Z">
<meta property="article:author" content="Jun Cao">
<meta property="article:tag" content="svm">
<meta property="article:tag" content="线性分类">
<meta property="article:tag" content="loss function">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i2.wp.com/dataaspirant.com/wp-content/uploads/2017/01/Support-vector-machine-svm.jpg?w=1024&ssl=1"><link rel="shortcut icon" href="data:image/png;base64,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"><link rel="canonical" href="http://example.com/2022/10/29/CS231n/CS231n%203%EF%BC%9Asvm%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%99%A8/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'CS231n 2:线性分类器',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-10-31 00:29:23'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://marketplace.canva.cn/EAEsSrIavXA/2/0/1600w/canva-lqyXLROcj-k.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/%20category/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">炸牛奶超级甜</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/%20category/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">CS231n 2:线性分类器</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-10-29T07:17:45.000Z" title="发表于 2022-10-29 15:17:45">2022-10-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-10-30T16:29:23.813Z" title="更新于 2022-10-31 00:29:23">2022-10-31</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="CS231n 2:线性分类器"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><h1>线性分类器</h1>
<h2 id="线性分类器简介">线性分类器简介</h2>
<h3 id="回顾Knn">回顾Knn</h3>
<p>图像分类的任务，就是从已有的固定分类标签集合中选择一个并分配给一张图像。cs231n中还介绍了k-Nearest Neighbor （k-NN）分类器，该分类器的基本思想是通过将测试图像与训练集带标签的图像进行比较，来给测试图像打上分类标签。k-Nearest Neighbor分类器存在以下不足：</p>
<ul>
<li>分类器只是简单储存所有训练数据，然后和未来测试数据比较。没有处理的数据直接储存非常浪费。</li>
<li>对一个测试图像进行分类需要和所有训练图像作比较，算法计算资源耗费高。</li>
</ul>
<p>概述。我们现在要开发一个更强大图像分类的函数。这个函数将包括两个主要部分:</p>
<p><strong>得分函数(score function)</strong>：将原始数据映射到班级分数</p>
<p>**损失函数(loss function)：**量化预测得分和地面真相标签之间的一致性。</p>
<p>现在就已经将这个得分问题转换为一个最优化问题，也就是最小化这个的损失函数。</p>
<h3 id="从图像到标签分值的参数化映射">从图像到标签分值的参数化映射</h3>
<p><strong>得分函数</strong>，这个函数将图像的像素值映射为各个分类类别的得分，得分高低仅代表图像属于该类别的可能性高低。</p>
<p>看一个具体的例子。</p>
<p>举例来说，在CIFAR-10中，我们有一个<strong>N</strong>=50000的训练集，每个图像有<strong>D</strong>=32x32x3=3072个像素，而<strong>K</strong>=10，这是因为图片被分为10个不同的类别（狗，猫，汽车等）。现在得分函数是：$f:R^D→R^K$，该函数是原始图像像素到分类分值的映射。</p>
<p>简单来说就是输入的是像素值输出的是最后的得分。</p>
<p>下面是cs231n给出的精确描述</p>
<p>现在假设有一个包含很多图像的训练集$x_i∈R^D$，每个图像都有一个对应的分类标签$y_i$。这里$i=1,2…N$并且$y_i∈1…K$。这就是说，我们有<strong>N</strong>个图像样例，每个图像的维度是<strong>D</strong>，共有<strong>K</strong>种不同的分类。</p>
<h3 id="线性分类器">线性分类器</h3>
<p>一个最简单的线性映射：</p>
<p>$f(x_i,W,b)=Wx_i+b$</p>
<p>在上面的公式中，假设每个图像数据都被拉长为一个长度为D的列向量，大小为[D x 1]。其中大小为[K x D]的矩阵<strong>W</strong>和大小为[K x 1]列向量<strong>b</strong>为该函数的<strong>参数</strong>。</p>
<p>这里可以看一以CIFAR-10的例子，$x_i$就包含了第$i$个图像的所有像素信息，这些信息被拉成为一个[3072 x 1]的列向量，<strong>W</strong>大小为[10x3072]，<strong>b</strong>的大小为[10x1]。因此，3072个数字（原始像素数值）输入函数，函数输出10个数字（不同分类得到的分值）。参数<strong>W</strong>被称为<strong>权重（weights）</strong>。<strong>b</strong>被称为<strong>偏差向量（bias vector）</strong>，这是因为它影响输出数值，但是并不和原始数据xi产生关联。在实际情况中，人们常常混用<strong>权重</strong>和<strong>参数</strong>这两个术语。</p>
<p><strong>需要注意的几点：</strong></p>
<ul>
<li>首先，一个单独的矩阵乘法$Wx_i$就高效地并行评估10个不同的分类器（每个分类器针对一个分类），其中每个类的分类器就是W的一个行向量。</li>
<li>注意我们认为输入数据$(x_i,y_i)$是给定且不可改变的，但参数<strong>W</strong>和<strong>b</strong>是可控制改变的。我们的目标就是通过设置这些参数，使得计算出来的分类分值情况和训练集中图像数据的真实类别标签相符。</li>
<li>该方法的一个优势是训练数据是用来学习到参数<strong>W</strong>和<strong>b</strong>的，一旦训练完成，训练数据就可以丢弃，留下学习到的参数即可。这是因为一个测试图像可以简单地输入函数，并基于计算出的分类分值来进行分类。</li>
<li>最后，注意只需要做一个矩阵乘法和一个矩阵加法就能对一个测试数据分类，这比k-NN中将测试图像和所有训练数据做比较的方法快多了。</li>
</ul>
<h3 id="理解线性分类器">理解线性分类器</h3>
<h4 id="理解W的作用">理解W的作用</h4>
<p>线性分类器计算图像中3个颜色通道中所有像素的值与权重的矩阵乘，简单来说就是将图片拉伸成为一个列向量然后和W进行相乘，从而得到分类分值。根据我们对权重设置的值，对于图像中的某些位置的某些颜色，函数表现出喜好或者厌恶。</p>
<p>有一个简单的例子，可以想象“船”分类就是被大量的蓝色所包围（对应的就是水）。那么“船”分类器在蓝色通道上的权重就有很多的正权重（它们的出现提高了“船”分类的分值），而在绿色和红色通道上的权重为负的就比较多（它们的出现降低了“船”分类的分值）。</p>
<p><img src="https://caojun-2014.oss-cn-beijing.aliyuncs.com/image-20221029154825917.png" alt="image-20221029154825917"></p>
<p>一个将图像映射到分类分值的例子。为了便于可视化，假设图像只有4个像素（都是黑白像素，这里不考虑RGB通道），有3个分类（红色代表猫，绿色代表狗，蓝色代表船，注意，这里的红、绿和蓝3种颜色仅代表分类，和RGB通道没有关系）。首先将图像像素拉伸为一个列向量，与W进行矩阵乘，然后得到各个分类的分值。需要注意的是，这个W一点也不好：猫分类的分值非常低。从上图来看，算法倒是觉得这个图像是一只狗。</p>
<h4 id="理解空间中的分类">理解空间中的分类</h4>
<p>将图像看做高维度的点：既然图像被伸展成为了一个高维度的列向量，那么我们可以把图像看做这个高维度空间中的一个点（即每张图像是3072维空间中的一个点）。整个数据集就是一个点的集合，每个点都带有1个分类标签。</p>
<p>既然定义每个分类类别的分值是权重和图像的矩阵乘，那么每个分类类别的分数就是这个空间中的一个线性函数的函数值。我们没办法可视化3072维空间中的线性函数，但假设把这些维度挤压到二维，那么就可以看看这些分类器在做什么了：</p>
<p><img src="https://caojun-2014.oss-cn-beijing.aliyuncs.com/image-20221029155130346.png" alt="image-20221029155130346"></p>
<p>图像空间的示意图。其中每个图像是一个点，有3个分类器。以红色的汽车分类器为例，红线表示空间中汽车分类分数为0的点的集合，红色的箭头表示分值上升的方向。所有红线右边的点的分数值均为正，且线性升高。红线左边的点分值为负，且线性降低。</p>
<p>从上面可以看到，<strong>W</strong>的每一行都是一个分类类别的分类器。<strong>对于这些数字的几何解释是：如果改变其中一行的数字，会看见分类器在空间中对应的直线开始向着不同方向旋转</strong>。而偏差<strong>b</strong>，则允许分类器对应的直线平移。需要注意的是，如果没有偏差，无论权重如何，在xi=0时分类分值始终为0。这样所有分类器的线都不得不穿过原点。</p>
<h4 id="将线性分类器看做模板匹配"><strong>将线性分类器看做模板匹配</strong></h4>
<p>关于权重<strong>W</strong>的另一个解释是<strong>它</strong>的每一行对应着一个分类的模板（有时候也叫作<em>原型</em>）。一张图像对应不同分类的得分，是通过使用内积（也叫<em>点积</em>）来比较图像和模板，然后找到和哪个模板最相似。</p>
<p>这里需要注意一下线性代数的相关知识，两个向量相乘的结果可以表示这两个向量之间的相似程度，也就是说W和图像相乘就是代表图像和哪一个类别更相似。</p>
<p>从这个角度来看，线性分类器就是在利用学习到的模板，针对图像做模板匹配。从另一个角度来看，可以认为还是在高效地使用k-NN，不同的是我们没有使用所有的训练集的图像来比较，而是每个类别只用了一张图片（这张图片是我们学习到的，而不是训练集中的某一张），<strong>而且我们会使用（负）内积来计算向量间的距离，而不是使用L1或者L2距离</strong>。</p>
<p><img src="https://caojun-2014.oss-cn-beijing.aliyuncs.com/templates.jpg" alt="image-20221029155130346"></p>
<p>将课程进度快进一点。这里展示的是以CIFAR-10为训练集，学习结束后的权重的例子。注意，船的模板如期望的那样有很多蓝色像素。如果图像是一艘船行驶在大海上，那么这个模板利用内积计算图像将给出很高的分数。</p>
<p>可以看到马的模板看起来似乎是两个头的马，这是因为训练集中的马的图像中马头朝向各有左右造成的。线性分类器将这两种情况融合到一起了。类似的，汽车的模板看起来也是将几个不同的模型融合到了一个模板中，并以此来分辨不同方向不同颜色的汽车。这个模板上的车是红色的，这是因为CIFAR-10中训练集的车大多是红色的。线性分类器对于不同颜色的车的分类能力是很弱的，但是后面可以看到神经网络是可以完成这一任务的。神经网络可以在它的隐藏层中实现中间神经元来探测不同种类的车（比如绿色车头向左，蓝色车头向前等）。而下一层的神经元通过计算不同的汽车探测器的权重和，将这些合并为一个更精确的汽车分类分值。</p>
<h4 id="偏差和权重的合并技巧："><strong>偏差和权重的合并技巧</strong>：</h4>
<p>在进一步学习前，要提一下这个经常使用的技巧。它能够将我们常用的参数W和b合二为一。回忆一下，分类评分函数定义为：</p>
<p>$f(x_i,W,b)=Wx_i+b$</p>
<p>分开处理这两个参数（权重参数W和偏差参数b）有点笨拙，一般常用的方法是把两个参数放到同一个矩阵中，同时$x_i$向量就要增加一个维度，这个维度的数值是常量1，这就是默认的<em>偏差维度</em>。这样新的公式就简化成下面这样：</p>
<p>$f(x_i,W)=Wx_i$</p>
<p>从数据维度来看也就是说</p>
<p>$x_i$的维度从[3072×1]变成了[3073×1]，W大小就是[10x3073]了。</p>
<p><img src="https://caojun-2014.oss-cn-beijing.aliyuncs.com/image-20221029160734303.png" alt="image-20221029160734303"></p>
<p>偏差技巧的示意图。左边是先做矩阵乘法然后做加法，右边是将所有输入向量的维度增加1个含常量1的维度，并且在权重矩阵中增加一个偏差列，最后做一个矩阵乘法即可。左右是等价的。通过右边这样做，我们就只需要学习一个权重矩阵，而不用去学习两个分别装着权重和偏差的矩阵了。</p>
<h3 id="图像数据预处理"><strong>图像数据预处理</strong></h3>
<p>在上面的例子中，所有图像都是使用的原始像素值（从0到255）。在机器学习中，对于输入的特征做归一化（normalization）处理是常见的套路。而在图像分类的例子中，图像上的每个像素可以看做一个特征。在实践中，对每个特征减去平均值来<strong>中心化</strong>数据是非常重要的。在这些图片的例子中，该步骤意味着根据训练集中所有的图像计算出一个平均图像值，然后每个图像都减去这个平均值，这样图像的像素值就大约分布在[-127, 127]之间了。下一个常见步骤是，让所有数值分布的区间变为[-1, 1]。<strong>零均值的中心化</strong>是很重要的，等我们理解了梯度下降后再来详细解释。</p>
<h2 id="损失函数">损失函数</h2>
<p>在线性分类器定义了从图像像素值到所属类别的评分函数（score function），该函数的参数是权重矩阵W。在函数中，数据$(x_i,y_i)$是给定的，不能修改。但是我们可以调整权重矩阵这个参数，使得评分函数的结果与训练数据集中图像的真实类别一致，即评分函数在正确的分类的位置应当得到最高的评分（score）。</p>
<p>回到之前那张猫的图像分类例子，它有针对“猫”，“狗”，“船”三个类别的分数。我们看到例子中权重值非常差，因为猫分类的得分非常低（-96.8），而狗（437.9）和船（61.95）比较高。我们将使用<strong>损失函数Loss Function）</strong>（有时也叫<strong>代价函数</strong>Cost Function<strong>或</strong>目标函数**）来衡量我们对结果的不满意程度。</p>
<p>简单的说，当得分函数输出结果与真实结果之间差异越大，损失函数输出越大，反之越小。</p>
<h3 id="多类支持向量机损失">多类支持向量机损失</h3>
<p>Multiclass Support Vector Machine Loss</p>
<p>损失函数的具体形式多种多样。首先，介绍常用的多类支持向量机（SVM）损失函数。SVM的损失函数想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值Δ。</p>
<p>我们可以把损失函数想象成一个人，这位SVM先生（或者女士）对于结果有自己的品位，如果某个结果能使得损失值更低，那么SVM就更加喜欢它。</p>
<p>让我们更精确一些。回忆一下，第i个数据中包含图像xi的像素和代表正确类别的标签yi。评分函数输入像素数据，然后通过公式f(xi,W)来计算不同分类类别的分值。这里我们将分值简写为s。比如，针对第j个类别的得分就是第j个元素：$s_j=f(x_i,W)j$。针对第i个数据的多类SVM的损失函数定义如下</p>
<p>$L_i=\sum_{j!=y_i}{\max(0,s_j-s_{y_i})+\Delta}$</p>
<p>这里直接看公式直接懵逼了，这个是啥，来看一个例子</p>
<p>假设有3个分类，并且得到了分值s=[13,−7,11]。</p>
<p>其中第一个类别是正确类别，即$y_i=0$。同时假设$\Delta$是10（后面会详细介绍该超参数）。上面的公式是将所有不正确分类（$j \neq y_i$）加起来，所以我们得到两个部分：</p>
<p>$Li=max(0,−7−13+10)+max(0,11−13+10)$</p>
<p>可以看到第一个部分结果是0，这是因为[-7-13+10]得到的是负数，经过max(0,−)函数处理后得到0。这一对类别分数和标签的损失值是0，这是因为正确分类的得分13与错误分类的得分-7的差为20，高于边界值10。而SVM只关心差距至少要大于10，更大的差值还是算作损失值为0。第二个部分计算[11-13+10]得到8。虽然正确分类的得分比不正确分类的得分要高（13&gt;11），但是比10的边界值还是小了，分差只有2，这就是为什么损失值等于8。</p>
<p><strong>==简而言之，SVM的损失函数想要正确分类类别$y_i$的分数比不正确类别分数高，而且至少要高$\Delta$。如果不满足这点，就开始计算损失值。==</strong></p>
<p>那么在这次的模型中，我们面对的是线性评分函数（f(xi,W)=Wxi），所以我们可以将损失函数的公式稍微改写一下：</p>
<p>$L_i=\sum_{j≠yi}max(0,w_jTx_i−w_{y_i}^T x_i+\Delta)$</p>
<p>其中$w_j$是权重W的第j行，被变形为列向量。然而，一旦开始考虑更复杂的评分函数f公式，这样做就不是必须的了。</p>
<p>在结束这一小节前，还必须提一下的属于是关于0的阀值：$max(0,−)$函数，它常被称为<strong>折叶损失（hinge loss）</strong>。有时候会听到人们使用平方折叶损失SVM（即L2-SVM），它使用的是$max(0,−)^2$，将更强烈（平方地而不是线性地）地惩罚过界的边界值。不使用平方是更标准的版本，但是在某些数据集中，平方折叶损失会工作得更好。可以通过交叉验证来决定到底使用哪个。</p>
<blockquote>
<p>我们对于预测训练集数据分类标签的情况总有一些不满意的，而损失函数就能将这些不满意的程度量化。</p>
</blockquote>
<p><img src="https://caojun-2014.oss-cn-beijing.aliyuncs.com/image-20221029182122953.png" alt="image-20221029182122953"></p>
<p>多类SVM“想要”正确类别的分类分数比其他不正确分类类别的分数要高，而且至少高出$\Delta$的边界值。如果其他分类分数进入了红色的区域，甚至更高，那么就开始计算损失。如果没有这些情况，损失值为0。我们的==目标是找到一个合适的W==，它们既能够让训练集中的数据样例满足这些限制，也能让总的损失值尽可能地低。</p>
<h4 id="正则化（Regularization）"><strong>正则化（Regularization）</strong></h4>
<p>上面损失函数有一个问题。假设有一个数据集和一个权重集<strong>W</strong>能够正确地分类每个数据（即所有的边界都满足，对于所有的i都有$L_i=0$）。问题在于这个<strong>W</strong>并不唯一：可能有很多相似的<strong>W</strong>都能正确地分类所有的数据。一个简单的例子：如果<strong>W</strong>能够正确分类所有数据，即对于每个数据，损失值都是0。==那么当$\lambda &gt;1$时，任何数乘W都能使得损失值为0，因为这个变化将所有分值的大小都均等地扩大了==，所以它们之间的绝对差值也扩大了。</p>
<p>举个例子，如果一个正确分类的分值和举例它最近的错误分类的分值的差距是15，对<strong>W</strong>乘以2将使得差距变成30。</p>
<p>换句话说，我们希望能向某些特定的权重<strong>W</strong>添加一些偏好，对其他权重则不添加，以此来消除模糊性。这一点是能够实现的，方法是向损失函数增加一个<strong>正则化惩罚（regularization penalty）</strong>$R(W)$部分。最常用的正则化惩罚是L2范式，L2范式通过对所有参数进行逐元素的平方惩罚来抑制大数值的权重：</p>
<p>$R(W)=∑_k∑_lW_{k,l}^2$</p>
<p>上面的表达式中，将W中所有元素平方后求和。注意==正则化函数不是数据的函数，仅基于权重==。包含正则化惩罚后，就能够给出完整的多类SVM损失函数了，它由两个部分组成：<strong>数据损失（data loss）</strong>，即所有样例的的平均损失Li，以及<strong>正则化损失（regularization loss）</strong>。完整公式如下所示：</p>
<p>$L=\frac{1}{N} \sum_i L_i + \lambda R(W) $</p>
<p>除了上述理由外，引入正则化惩罚还带来很多良好的性质，这些性质大多会在后续章节介绍。比如引入了L2惩罚后，SVM们就有了**最大边界（**max margin这一良好性质。（如果感兴趣，可以查看<a href="https://link.zhihu.com/?target=http%3A//cs229.stanford.edu/notes/cs229-notes3.pdf">CS229课程</a>）。</p>
<p>其中最好的性质就是对大数值权重进行惩罚，可以提升其泛化能力，因为这就意味着没有哪个维度能够独自对于整体分值有过大的影响。举个例子，假设输入向量$x=[1,1,1,1]$，两个权重向量$w_1=[1,0,0,0]$，$w_2=[0.25,0.25,0.25,0.25]$。那么$w_1^Tx=w_2^T=1$，两个权重向量都得到同样的内积，但是$w_1$的$L2$惩罚是1.0，而$w_2$的$L2$惩罚是0.25。因此，根据$L2$惩罚来看，$w_2$更好，因为它的正则化损失更小。从直观上来看，这是因为$w_2$的权重值更小且更分散。既然$L2$惩罚倾向于更小更分散的权重向量，这就会鼓励分类器最终将所有维度上的特征都用起来，而不是强烈依赖其中少数几个维度。在后面的课程中可以看到，这一效果将会提升分类器的泛化能力，并避免<em>过拟合</em>。</p>
<p>需要注意的是，和权重不同，偏差没有这样的效果，因为它们并不控制输入维度上的影响强度。因此通常只对权重W正则化，而不正则化偏差b。在实际操作中，可发现这一操作的影响可忽略不计。最后，因为正则化惩罚的存在，不可能在所有的例子中得到0的损失值，这是因为只有当W=0的特殊情况下，才能得到损失值为0。</p>
<h4 id="实际考虑设置-Delta">实际考虑设置$\Delta$</h4>
<p>你可能注意到上面的内容对超参数$\Delta$及其设置是一笔带过，那么它应该被设置成什么值？需要通过交叉验证来求得吗？现在看来，该超参数在绝大多数情况下设为Δ=1.0都是安全的。超参数Δ和λ看起来是两个不同的超参数，但实际上他们一起控制同一个权衡：==即损失函数中的数据损失和正则化损失之间的权衡==。理解这一点的关键是要知道，权重W的大小对于分类分值有直接影响（当然对他们的差异也有直接影响）：当我们将W中值缩小，分类分值之间的差异也变小，反之亦然。因此，不同分类分值之间的边界的具体值（比如Δ=1或Δ=100）从某些角度来看是没意义的，因为权重自己就可以控制差异变大和缩小。也就是说，真正的权衡是我们允许权重能够变大到何种程度（通过正则化强度λ来控制）。</p>
<h3 id="softmax分类器">softmax分类器</h3>
<p>SVM是最常用的两个分类器之一，而另一个就是<strong>Softmax分类器，<strong>它的损失函数与SVM的损失函数不同。对于学习过二元逻辑回归分类器的读者来说，Softmax分类器就可以理解为逻辑回归分类器面对多个分类的一般化归纳。SVM将输出$f(x_i,W)$作为每个分类的评分（因为无定标，所以难以直接解释）。与SVM不同，Softmax的输出（归一化的分类概率）更加直观，并且从概率上可以解释，这一点后文会讨论。在Softmax分类器中，函数映射$f(x_i;W)=Wx_i$保持不变，但将这些评分值视为每个分类的未归一化的对数概率，并且将*折叶损失（hinge loss）*替换为</strong>交叉熵损失</strong>（<strong>cross-entropy loss）</strong>。公式如下：</p>
<p>$L_i=-log(\frac{e^{f_{y_i}}}{\sum_j e^{f_{j}}})$</p>
<p>在上式中，使用$f_j$来表示分类评分向量$f$中的第$j$个元素。和之前一样，整个数据集的损失值是数据集中所有样本数据的损失值Li的均值与正则化损失$R(W)$之和。其中函数$f_j(z)$被称作<strong>softmax 函数</strong>：其输入值是一个向量，向量中元素为任意实数的评分值（z中的），函数对其进行压缩，==输出一个向量，其中每个元素值在0到1之间，且所有元素之和为1==。所以，包含softmax函数的完整交叉熵损失看起唬人，实际上还是比较容易理解的。</p>
<p>对于交叉熵还有信息论和概率论方向的理解，这里暂时写了。</p>
<h4 id="实操事项："><strong>实操事项：</strong></h4>
<p>数值稳定。编程实现softmax函数计算的时候，中间项$e^{f_{y_i}}$和$∑_je^{f_j}$因为存在指数函数，所以数值可能非常大。除以大数值可能导致数值计算的不稳定，所以学会使用归一化技巧非常重要。如果在分式的分子和分母都乘以一个常数C，并把它变换到求和之中，就能得到一个从数学上等价的公式：</p>
<p>$\frac {^{fy_i} } {∑_je^{f_j} }=\frac {Ce^{f_{y_i} } } {C∑_j e^{f_j} }=\frac {e^{f_{y_i}+logC} } {∑_j e^{ {f_j}+logC} }$</p>
<p>C的值可自由选择，不会影响计算结果，通过使用这个技巧可以提高计算中的数值稳定性。通常将C设为$logC=−max f_j$。该技巧简单地说，就是应该将向量f中的数值进行平移也就是所有的数值都减去最大值，使得最大值为0。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">f = np.array([<span class="number">123</span>, <span class="number">456</span>, <span class="number">789</span>]) <span class="comment"># 例子中有3个分类，每个评分的数值都很大</span></span><br><span class="line">p = np.exp(f) / np.<span class="built_in">sum</span>(np.exp(f)) <span class="comment"># 不妙：数值问题，可能导致数值爆炸</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 那么将f中的值平移到最大值为0：</span></span><br><span class="line">f -= np.<span class="built_in">max</span>(f) <span class="comment"># f becomes [-666, -333, 0]</span></span><br><span class="line">p = np.exp(f) / np.<span class="built_in">sum</span>(np.exp(f)) <span class="comment"># 现在OK了，将给出正确结果</span></span><br></pre></td></tr></table></figure>
<p><strong>让人迷惑的命名规则</strong>：精确地说，SVM分类器使用的是<em>折叶损失（hinge loss）</em>，有时候又被称为<em>最大边界损失（max-margin loss）</em>。Softmax分类器使用的是<em>交叉熵损失（corss-entropy loss）</em>。Softmax分类器的命名是从<em>softmax函数</em>那里得来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。注意从技术上说“softmax损失（softmax loss）”是没有意义的，因为softmax只是一个压缩数值的函数。但是在这个说法常常被用来做简称。</p>
<h3 id="SVM和Softmax的比较">SVM和Softmax的比较</h3>
<p>下图有助于区分这 Softmax和SVM这两种分类器：</p>
<p><img src="https://caojun-2014.oss-cn-beijing.aliyuncs.com/image-20221029194512543.png" alt="image-20221029194512543"></p>
<p>针对一个数据点，SVM和Softmax分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量<strong>f</strong>（本节中是通过矩阵乘来实现）。不同之处在于对<strong>f</strong>中分值的解释：</p>
<p>SVM分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别2）的分值比其他分类的分值高出至少一个边界值。</p>
<p>Softmax分类器将这些数值看做是每个分类没有归一化的<strong>对数概率</strong>，鼓励正确分类的归一化的对数概率变高，其余的变低。SVM的最终的损失值是1.58，Softmax的最终的损失值是0.452，==但要注意这两个数值没有可比性。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义==。</p>
<h3 id="Softmax分类器为每个分类提供了“可能性”"><strong>Softmax分类器为每个分类提供了“可能性”</strong></h3>
<p>SVM的计算是无标定的，而且难以针对所有分类的评分值给出直观解释。Softmax分类器则不同，它允许我们计算出对于所有分类标签的可能性。举个例子，针对给出的图像，SVM分类器可能给你的是一个[12.5, 0.6, -23.0]对应分类“猫”，“狗”，“船”。而softmax分类器可以计算出这三个标签的”可能性“是[0.9, 0.09, 0.01]，这就让你能看出对于不同分类准确性的把握。为什么我们要在”可能性“上面打引号呢？这是因为可能性分布的集中或离散程度是由正则化参数λ直接决定的，λ是你能直接控制的一个输入参数。举个例子，假设3个分类的原始分数是[1, -2, 0]，那么softmax函数就会计算：</p>
<p>​		$[1,−2,0]→[e^1,e^{−2},e^0]=[2.71,0.14,1]→[0.7,0.04,0.26]$</p>
<p>现在，如果正则化参数λ更大，那么权重W就会被惩罚的更多，然后他的权重数值就会更小。这样算出来的分数也会更小，假设小了一半吧[0.5, -1, 0]，那么softmax函数的计算就是：</p>
<p>$[0.5,−1,0]→[e^0.5,e^{−1},e^0]=[1.65,0.73,1]→[0.55,0.12,0.33]$</p>
<p>现在看起来，概率的分布就更加分散了。还有，随着正则化参数$λ$不断增强，权重数值会越来越小，最后输出的概率会接近于均匀分布。这就是说，softmax分类器算出来的概率最好是看成一种对于分类正确性的自信。和SVM一样，数字间相互比较得出的大小顺序是可以解释的，但其绝对值则难以直观解释。</p>
<p><strong>在实际使用中，SVM和Softmax经常是相似的</strong>：通常说来，两种分类器的表现差别很小，不同的人对于哪个分类器更好有不同的看法。相对于Softmax分类器，SVM更加“局部目标化（local objective）”，这既可以看做是一个特性，也可以看做是一个劣势。考虑一个评分是[10, -2, 3]的数据，其中第一个分类是正确的。那么一个SVM（Δ=1）会看到正确分类相较于不正确分类，已经得到了比边界值还要高的分数，它就会认为损失值是0。==SVM对于数字个体的细节是不关心的==：如果分数是[10, -100, -100]或者[10, 9, 9]，对于SVM来说没设么不同，只要满足超过边界值等于1，那么损失值就等于0。</p>
<p>对于softmax分类器，情况则不同。对于[10, 9, 9]来说，计算出的损失值就远远高于[10, -100, -100]的。换句话来说，softmax分类器对于分数是永远不会满意的：==正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小。但是，SVM只要边界值被满足了就满意了，不会超过限制去细微地操作具体分数。这可以被看做是SVM的一种特性==。举例说来，一个汽车的分类器应该把他的大量精力放在如何分辨小轿车和大卡车上，而不应该纠结于如何与青蛙进行区分，因为区分青蛙得到的评分已经足够低了。</p>
<h2 id="小结">小结</h2>
<p>总结如下：</p>
<ul>
<li>定义了从图像像素映射到不同类别的分类评分的评分函数。在本节中，评分函数是一个基于权重<strong>W</strong>和偏差<strong>b</strong>的线性函数。</li>
<li>与kNN分类器不同，<strong>参数方法</strong>的优势在于一旦通过训练学习到了参数，就可以将训练数据丢弃了。同时该方法对于新的测试数据的预测非常快，因为只需要与权重<strong>W</strong>进行一个矩阵乘法运算。</li>
<li>介绍了偏差技巧，让我们能够将偏差向量和权重矩阵合二为一，然后就可以只跟踪一个矩阵。</li>
<li>定义了损失函数（介绍了SVM和Softmax线性分类器最常用的2个损失函数）。==损失函数能够衡量给出的参数集与训练集数据真实类别情况之间的一致性==。在损失函数的定义中可以看到，对训练集数据做出良好预测与得到一个足够低的损失值这两件事是等价的。</li>
</ul>
<p>现在我们知道了如何基于参数，将数据集中的图像映射成为分类的评分，也知道了两种不同的损失函数，它们都能用来衡量算法分类预测的质量。但是，如何高效地得到能够使损失值最小的参数呢？这个求得最优参数的过程被称为最优化，将在下节课中进行介绍。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Jun Cao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2022/10/29/CS231n/CS231n%203%EF%BC%9Asvm%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%99%A8/">http://example.com/2022/10/29/CS231n/CS231n%203%EF%BC%9Asvm%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%99%A8/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">炸牛奶超级甜</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/svm/">svm</a><a class="post-meta__tags" href="/tags/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/">线性分类</a><a class="post-meta__tags" href="/tags/loss-function/">loss function</a></div><div class="post_share"><div class="social-share" data-image="https://i2.wp.com/dataaspirant.com/wp-content/uploads/2017/01/Support-vector-machine-svm.jpg?w=1024&amp;ssl=1" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/10/30/CS231n/CS231n%205%EF%BC%9A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AC%94%E8%AE%B0/"><img class="prev-cover" src="http://www.tensorflownews.com/wp-content/uploads/2018/03/%E5%9B%BE%E7%89%8716.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">CS231n 4:反向传播</div></div></a></div><div class="next-post pull-right"><a href="/2022/10/29/HDFS%E7%9A%84shell%E5%91%BD%E4%BB%A4/"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">HDFS的shell命令</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://marketplace.canva.cn/EAEsSrIavXA/2/0/1600w/canva-lqyXLROcj-k.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Jun Cao</div><div class="author-info__description">斗气大陆第一风投师 ,一年一千算法题</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">线性分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">线性分类器简介</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E9%A1%BEKnn"><span class="toc-number">1.1.1.</span> <span class="toc-text">回顾Knn</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E5%9B%BE%E5%83%8F%E5%88%B0%E6%A0%87%E7%AD%BE%E5%88%86%E5%80%BC%E7%9A%84%E5%8F%82%E6%95%B0%E5%8C%96%E6%98%A0%E5%B0%84"><span class="toc-number">1.1.2.</span> <span class="toc-text">从图像到标签分值的参数化映射</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">1.1.3.</span> <span class="toc-text">线性分类器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%90%86%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">1.1.4.</span> <span class="toc-text">理解线性分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%90%86%E8%A7%A3W%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">1.1.4.1.</span> <span class="toc-text">理解W的作用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%90%86%E8%A7%A3%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E5%88%86%E7%B1%BB"><span class="toc-number">1.1.4.2.</span> <span class="toc-text">理解空间中的分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%86%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8%E7%9C%8B%E5%81%9A%E6%A8%A1%E6%9D%BF%E5%8C%B9%E9%85%8D"><span class="toc-number">1.1.4.3.</span> <span class="toc-text">将线性分类器看做模板匹配</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%81%8F%E5%B7%AE%E5%92%8C%E6%9D%83%E9%87%8D%E7%9A%84%E5%90%88%E5%B9%B6%E6%8A%80%E5%B7%A7%EF%BC%9A"><span class="toc-number">1.1.4.4.</span> <span class="toc-text">偏差和权重的合并技巧：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.1.5.</span> <span class="toc-text">图像数据预处理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.</span> <span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E7%B1%BB%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%8D%9F%E5%A4%B1"><span class="toc-number">1.2.1.</span> <span class="toc-text">多类支持向量机损失</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88Regularization%EF%BC%89"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">正则化（Regularization）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E9%99%85%E8%80%83%E8%99%91%E8%AE%BE%E7%BD%AE-Delta"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">实际考虑设置$\Delta$</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#softmax%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">1.2.2.</span> <span class="toc-text">softmax分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E6%93%8D%E4%BA%8B%E9%A1%B9%EF%BC%9A"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">实操事项：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SVM%E5%92%8CSoftmax%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-number">1.2.3.</span> <span class="toc-text">SVM和Softmax的比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Softmax%E5%88%86%E7%B1%BB%E5%99%A8%E4%B8%BA%E6%AF%8F%E4%B8%AA%E5%88%86%E7%B1%BB%E6%8F%90%E4%BE%9B%E4%BA%86%E2%80%9C%E5%8F%AF%E8%83%BD%E6%80%A7%E2%80%9D"><span class="toc-number">1.2.4.</span> <span class="toc-text">Softmax分类器为每个分类提供了“可能性”</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">1.3.</span> <span class="toc-text">小结</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/11/04/%E5%85%AB%E8%82%A1%E6%96%87/%E4%B8%BA%E4%BB%80%E4%B9%88%20MySQL%20%E9%87%87%E7%94%A8%20B+%20%E6%A0%91%E4%BD%9C%E4%B8%BA%E7%B4%A2%E5%BC%95%EF%BC%9F/" title="为什么MySql采用B+树作为索引"><img src="https://caojun-2014.oss-cn-beijing.aliyuncs.com/b6678c667053a356f46fc5691d2f5878.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="为什么MySql采用B+树作为索引"/></a><div class="content"><a class="title" href="/2022/11/04/%E5%85%AB%E8%82%A1%E6%96%87/%E4%B8%BA%E4%BB%80%E4%B9%88%20MySQL%20%E9%87%87%E7%94%A8%20B+%20%E6%A0%91%E4%BD%9C%E4%B8%BA%E7%B4%A2%E5%BC%95%EF%BC%9F/" title="为什么MySql采用B+树作为索引">为什么MySql采用B+树作为索引</a><time datetime="2022-11-03T16:00:00.000Z" title="发表于 2022-11-04 00:00:00">2022-11-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/04/%E5%85%AB%E8%82%A1%E6%96%87/MySql%E7%B4%A2%E5%BC%95%E5%A4%B1%E6%95%88%E5%9C%BA%E6%99%AF/" title="索引失效"><img src="https://caojun-2014.oss-cn-beijing.aliyuncs.com/f287701eba9bf6f32a2d09b013bb451b.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="索引失效"/></a><div class="content"><a class="title" href="/2022/11/04/%E5%85%AB%E8%82%A1%E6%96%87/MySql%E7%B4%A2%E5%BC%95%E5%A4%B1%E6%95%88%E5%9C%BA%E6%99%AF/" title="索引失效">索引失效</a><time datetime="2022-11-03T16:00:00.000Z" title="发表于 2022-11-04 00:00:00">2022-11-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/02/transformer%E7%BB%93%E6%9E%84/" title="无题"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无题"/></a><div class="content"><a class="title" href="/2022/11/02/transformer%E7%BB%93%E6%9E%84/" title="无题">无题</a><time datetime="2022-11-02T08:02:21.753Z" title="发表于 2022-11-02 16:02:21">2022-11-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/01/leetCode/202211%E5%88%B7%E9%A2%98/" title="a"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="a"/></a><div class="content"><a class="title" href="/2022/11/01/leetCode/202211%E5%88%B7%E9%A2%98/" title="a">a</a><time datetime="2022-11-01T07:34:43.000Z" title="发表于 2022-11-01 15:34:43">2022-11-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/10/30/CS231n/CS231n%204%EF%BC%9A%E6%9C%80%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/" title="CS231n 3:最优化"><img src="https://img-blog.csdnimg.cn/20181222203042133" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CS231n 3:最优化"/></a><div class="content"><a class="title" href="/2022/10/30/CS231n/CS231n%204%EF%BC%9A%E6%9C%80%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/" title="CS231n 3:最优化">CS231n 3:最优化</a><time datetime="2022-10-29T16:00:00.000Z" title="发表于 2022-10-30 00:00:00">2022-10-30</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Jun Cao</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a target="_blank" rel="noopener" href="https://butterfly.js.org/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>