<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>最优化 | 炸牛奶超级甜</title><meta name="author" content="Jun Cao"><meta name="copyright" content="Jun Cao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="内容列表：  简介 损失函数可视化 最优化  策略#1：随机搜索 策略#2：随机局部搜索 策略#3：跟随梯度  梯度计算  使用有限差值进行数值计算 微分计算梯度  梯度下降 小结  最优化 在上一节中，我们介绍了图像分类任务中的两个关键部分：  基于参数的评分函数。该函数将原始图像像素映射为分类评分值（例如：一个线性函数）。 损失函数。该函数能够根据分">
<meta property="og:type" content="article">
<meta property="og:title" content="最优化">
<meta property="og:url" content="http://example.com/2022/10/30/CS231n%204.1%EF%BC%9A%E6%9C%80%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%8A%EF%BC%89/index.html">
<meta property="og:site_name" content="炸牛奶超级甜">
<meta property="og:description" content="内容列表：  简介 损失函数可视化 最优化  策略#1：随机搜索 策略#2：随机局部搜索 策略#3：跟随梯度  梯度计算  使用有限差值进行数值计算 微分计算梯度  梯度下降 小结  最优化 在上一节中，我们介绍了图像分类任务中的两个关键部分：  基于参数的评分函数。该函数将原始图像像素映射为分类评分值（例如：一个线性函数）。 损失函数。该函数能够根据分">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20181222203042133">
<meta property="article:published_time" content="2022-10-29T16:00:00.000Z">
<meta property="article:modified_time" content="2022-10-30T11:50:28.604Z">
<meta property="article:author" content="Jun Cao">
<meta property="article:tag" content="最优化">
<meta property="article:tag" content="优化策略">
<meta property="article:tag" content="梯度下降">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20181222203042133"><link rel="shortcut icon" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOEAAADhCAMAAAAJbSJIAAAAilBMVEX29vYAAAABAQH4+Pj////8/Pz09PTv7+/t7e3x8fHq6uoFBQW1tbXn5+fa2trg4OAbGxvLy8vT09OWlpYNDQ1/f38gICBnZ2ecnJx1dXWnp6eFhYWLi4vExMQVFRVLS0tTU1M+Pj4lJSWwsLAzMzNubm5bW1tBQUFgYGChoaEtLS0/Pz+YmJhHR0dGYStRAAAZT0lEQVR4nNVdCWPjKg42FvhOczRJk7Rxc/Z60///9xYBAuzYTrudxh69nZ2ZjJ0iELqQPoLgrxIEgQiKl5KVz3/GAgL870vvbM8feX56wncGTZIhURwYY6H8xR7uxNfeEscJM7TOvvZObwSwZizPwzDMczneFf/CkohsJ6ckD5l6p5wNmUWA6BkXMFT/w4U8i+CKnIK4wzdwUvR7rOA3Gu63SYpo8sly5kgO96F7RSAQM1Z/5zjYzQiwU4vgj5atu1nUK+i9hH/eDlRQga/1VnKDxeG+8w5BhWxpHiPK8S93A1xEKaJ8pRYt1OuQq6GyzuFCwJ+1iMqn85CFtPSTGK7t39sT4IayQ9SD1iryo32s/F1vQvmYvxdz9jbArQjpieUkbcvZdmkWRw79vm1bwYimQz71OX53ksreh7cVxULbB9xUk5EQ8YdkUat/NoZm34afScuoVeOFp21GQ1tEmDoRkxzhms4ZyV6LyXDvMPYi5CzwDbM657+BWUWAg7MQWwERgJgyLbZyRZqVjVQzxJDaq5LFhZunzbDkVPxxy3GvPG6pWx+1jMqFfGlaEJi5d2Z6CiB6tRM1zwYkp4AqQy9Xzp7BjAyCg1EjymJcjJcb/0Ca0Eftv0oPZ8zMh6H0FYbDYiTOjGyhJ5GisGu0vlhEgDvSo6xM6cNI2o/cKGG5sENhEVDeQrPl7j0XRuyJQyly1dFKXhbWxXu3/MutuDcWUkrDUJQNaHlT42IT4bGCi5jrharrDQjiJbEvPRgr2MoTx1dQSU0Hs4ZTp+SnHieS9Yn2Mxl7rm0q8ET4vrJW3BhW+eu1y6W9IQH/tBy+VZdKbBiJ4l2dwzfLYVZhA0ZzRi7tdgA7EeVqax1SyUdlsEFMG5StqrxDZhl849UvFPda2cgXd8OwiWKnXVC5E89V3SDF9IHC2331n4yQotO2rayTnKIo165CzoahTtH3MlOu3LXKv8HWGPCQeQZc/kmybkzF/IIJKdvkDEl/r3cWIzlWo9/rS6jyGhPjCrBjReKENYaLerYKAOy0DMEBR3fG2OiLJcRN9UAcPnC3HGhBKSKsaxP5V7GyHD7x6BZcdBF/8lTG5YSLo7V6/hryP6Rj58nFSxAk9jtZ/66bcDmWWcNgYGz/2bMXQD5pKMOmhi+VTrumnBV9q1NSJcqoNz0gTvTv3kaEYE5r+KfpLTsvOdv17brxN6bjg1A6Zk1ryNfE4cKNFb1uY0ObPTP+Yq3+uFeDocImYyomiYx7L5+Qhs/I8aedAc9lmyeNX+x5Eas+DYbU6xsb5S6axUkulxnqxDFj91lrskKUpINfv3J+9VsE6M9QYDhrSXFyuxpO1UghNHQZOAbaI3giDmue4K1pZPXMoU0j8BPZdpush8iqn9ZsjNPBrcnI3ydQsQOz26X5IRfp2qGCm5i2EBAEZWzYiTfs7xsRmjWiC3+GnhHvJMj2kEapUk1tXhkYv6brq29AENlR7FtFCaxatMbdM6Jtsg0B5aQ6JPn3CVyc3iak2nrrNbQHGMLm7w+8VVHCM7muPRp9PE5jdT1ZJ4ituk2JQ+vLNiZS9WsyEKZjt/jvD/2LxA809lPQqtIh0mUI8ikKEflZfxK2GVH1nkng9XliSt6jHmn7Iu7JXJDKEC/MuAmdlmBJ7z31xSFGRpQyagtywPETWtMArzQzmw5rjidTmsNmn/7XCQJyqrtCcTwbPhOHhXbAIPjwPml9UU6g2eVlT0cYIJfCBAD7Lm2nEqA6kUHnL3Na+66cr9kE+CN6qrGh4xjWqTD8LIA52IXU7t+moNm9aJ7q7UQYk2wkbF0RjlT7RphXVQ6vuCvA34jDKyUrv0TojpHBGnc9KFY0ESZ9D4ldw85UmktIvfYSXkRk78N230sRzoRm6ck8l1kdnHW8B55z14vvDfBMP/+1m8NNjUPIrCvUxWHgpdx6cr5LZlRNp6LxQizicGSTMJ0cQmAP4IoexBQCd7bSreo617BL09gMAjr2PXDoFRpsO3+8iyRoHyY2sdGdtBe2NKNbSn6JvNCpe5e4WJb8S7f6nRx6UXAvB95eEJ52ZsPEH9K5lP2N9Wth2D03KhOpF3vSB4fOVVl263LxRByuLnyarjJL0AGUfrYXDs/E4RV7LB5pmCYdQckPL9poJkoPSPqLA/8ycWsOGw9XHAmbCTCRBAQTm2Ds5nCUW5X09wb+ZeI253nuzLtj9JRX+IHgRD5N98mSqiDWP6OPqmFOtXrdAb7UFybB7UXAnySlnWk0CNIP+hmdQcgvEZ/TT3/sNscuordZDFtpciWfDVZO+igeMtEbY9fSKNGSZJKOe8WaeO605NKpeSVz0b1hf4c4TS9y2LUP06ZsotleLx0Cjg++0oN9c9hpt6kykx1sRtg6C6dOOwfis1cObXX9U8cwQZVdaA4/7Qmpc/i6pRTs8UwvHNp+s263GNnRHNpybzEj7cq6nAVATWNmpxdNY61FdzEvnT2hzjUfNRdoXBL0ai2wItHQw2UBsPdnTHZoDm0YCZld/27pc885F/12rOpMGC7P7kKXRl79AdZm1jU+7OndLpMP3klqat+N01vx6KoNTtVxBUGaeMNwOrfpHL/bILqTVKG/GqI4jm+1jF58WB1WIMcQxxFQgpuemjvv2QXvu/YuU2nwXZuKyX+kcZTG8W04hLYYP4rTNMnkUMxzNtnhLbVLTh26tJR77FNzGMdpLOUjbX/lb5Iduh8DyYVLJINRnKTmWNOV7rnCPqB0OetOJgu7Ec7o+8gVlP+lOHs3WUVw2UyrLiDKpJaJszTK5FSrj1yjz6NX9eVC22m7qrFRiT5El8KfSOkIkiS5zaEwREtvhtUnchBxEuA0J3KvaH3qAmUXCwKWPNVnp+EnBPapLaRJmkVyAjPJI8S3kVN4IGelVBwC7sBA8pagPoDIjGKiJdk32qpDg0pQOs65x/QqG0nxjNJMyqmcQamsb1NWi/UGxtKp1HWayGmWVlrOtLMVRuHL53Jv3oHb6OKzfUuZLSzfXSbKAMUJJJcFt79HqlBGL0QhNaK08nGSSW2XBanT51STj8X63thEQRsxb01728NjbGBMM9zZcvaS2zk1ELgT0jOXMik1aJSkcpPE4DSBUYdhTRxtkU3XISnV/M3ZnwjNoBTNEdxIQDWJZxrlUjIn9WiUyh0YpSNPmYtn4vAoAEmA+t1VLLbWj2KzEcpIWbKZ3OFSOuQK3sjaG5KbiTo/tpGcXKnupCHM3H6TJizT2Rz5yPRuNjU0m92NdraGvxlYQjWU4vadH9iHtBPKj7h16RClpMtDvpZ7MJLmCiLPGssF4wYTIs8r0BBK9ExtcckbHTcI0gkL88NkPi/XALi105v53JaEyjHl5bwsRzHaP6lNA7MiIEQ8Xb3MCVGgxiD1CMv/TotiJMRFJbDSUWU5n8ylhKAexX14axaNzs8nS7ZBQw84y+hxS/buVi+kRb9An08zoToXvR2sKjvL5SEslQ3spbiNoptyvjxJ/ZkYNSd4vHnV+uUr/Bm0iP0q88QVAu32zuUarkWa9lO8p1pakY98XrJCqnG1T0CMlCsaaqyLL3AYEpPrkYs0qNRkjpo0hhuFExekKttQ4U3mJ1AraPmjoX+JRYO9IwPijELdGXFfvkod01cxO0A80XxIdbBRdk1s5h1shZpa/z1n8yMHZIdbO8qK/spLTb+nMhg5W0oZFbGBfGriruNvbgKkh6a61wrb2/YRRX22I0Bc0tEZW3DMOoRf23zN3Ku26MlYoKMQmjXc1HTsrYnfO1M+LfxB5ySO89PuvLjfFNutcmfQs9lui8394mF3mtPaVUR3hl2bhvWP3lgjgtwO5uANkorQn4q7LOJcKAJD0jdF4jJimB0fP62A0nrmrqex1nraB9kmLeSpKp/rbSo8Z8XKmvt/5FaIpDizOtE37b+C+PbLpMJ15doYoAAtcs+F4NACKwje78g38OD4n3pT7+HcMBkOAxMLQz3P6wwVPtR/U/Et6ZKCW5yU+xr634RnxP2ziHWmPoQZjvEdBLT3J7R8TXBv4LPom0K2773NWZPL0Wvhmt99t75HSTNwnUUN3U78IrTkDYi4Q1eUnZKrYIlNhA7RaGmkXEPb9d7lbMlLf+fsMPr/XRAxLs2eDq+VMdyWIPYsxk9aPj1Yu1zhZg6EgKrxcXTHH0w8tp+8W7SiSX+NlTUCV7QQ/rwQ1BWTdTTU3JjAR5b7sYX2vmwwOFF+G+LDz+uxvc7b4SC2Taypn/44lAMFNqBhtYYCu0dpU6nm938jVsVWdt1a2d5heluyXT+q7efHa6hylFrqy6E4bW+2NO9vwDo5fDt15DiArYiFS+YIalkBa/s/vy7AZhztuA0g/EWyZ/KhwSX7P3kDu/58RxwuhsAhdpgRMte7MJE7lwTfkVjKbwQqgWcaNFpQQW9MYJt+dG8BJmHuNo/n9dNxBNfDV8VSJER8V2w2xTjVlw1sKX+3vwUL1wgcfACTUYWI3vf0188jXB4p1V/HOdmeTc34ZDHjgHJvDqYmNzzTbifK/IWIy8W3Sx1Eaejy0/RKIgmz23efzDu8eMgECPuNA4Brk+bwk8az5LpeTZlro+9XvHM7goL0NrDsJmu65aYNIOwV1sQNET7IK30m+D2fukqkAx8iw5B8/8j3fTaS1Akgt4dji0omydB7F4v8vv64ar9/oD/20UhySdYcLnWepjrgrqDfIWJ7ayj/vrRQhINAoU2rLOkbSuymkrRvZdCWxkm9JDUTHnVUJmiYHLpLAOxgj6LFg7ZFuPaVipQPREqhziECBBTju5WtK3QI3xevUil+zk7H8d1mySolG0NJ6sesSljfhQcuyYeDXGgep3DwrW94MCVSq0OHxOHFPqTjItUNpF2Tllid6+ZSKZimFpoAlxyHA7CHF1LqMI+o0aI1tWu6mrwoqWodh8EheIhmhgh+XeOVq2xE4xpqS1pxzvys3VA4DBD0t2LRPmJdd4L5G2MoP1vCPMoNzFPjoIuxz+BA/FKAZZXDZWpOPlX1pPqohcPIZj8Sw6HXQqI+7ws+ySfdX1bh0fbCvlMQ1NbsbdeKzJ6DhkLK2eG25aQtJJ5rKv6J6zXkH7SGLckIsJ1paxNkcf+7woFEwDY+dCzOVAWMdKpD80FLQongyhh2XSCLfFO7oOyyZ6wPcj1MdvtsgfPUU/xtFt9hg7Ii4Dx6r1/B9jaAPI0DurK7Rw5yf344OBW7aznMcM138snT+eHDOajm00Hk2hTATletXsdhNYLrt9eIYQ59EBzCtovDvNVWBPqqmYaY2dEwDvLtjThtHHYAfzrYk2bqaPu6HUGQ5Sb3pGXLEzNVath+RAbqrh1bPUtv2btphuG0BQ7YWZqvNfNJ7bH7bjgCGWNVBRUrTNdGpZZ91T7XiFvskjKoWA7Uqo1Q9B5B9lnToOyBvxoOuxEXbkdiQccM7E4UlcG+zq6OEeCRVehdQGlOSIdhDp3hRlMvRLqyPfSvX8KRA3Hnrg1i6xF30Mr3A+GQgrpcbzrB745P68WfAtsnvqIpAHhcrJ/3+91CejYABI8dDubGVTvnuUZXwHYnrAoOvniUiAuNb6gTObyu7J5yA8NQpdgJ+WFUp06N+o0v3/si/Rt/IJszECG1+BFIfyEmR5wss48Hw6EDKP0b9TQO+KQ3kOsL8m6W+7mr7GE1DKauTcqVddauXMP9lS/TGUV0AoaQhlIEXj3h/OcVQ7pzOMR2oKFwqO5UJfp5Gl5nmPPw4o6zPklBGofG5/r5d5mzw6HU7QWoHDJGbsiPK2DoCp2+b9CpEoBFSmI/HZW9UeIUDaKozZB4sujOPz21tbgu5wF0PDnyTgLrtRd0HYIq7BK2h82Uh9U9O1A3l6rvGkbVniFA2DGjHy6u1EG/GlmLxtPiuNm8bzbH7XScCtPW5psEUOF03nms2hOBuWAupGNb7+Be8Gh8fNzZqNFQ+fJYjIOLeqKMLtU7tQhpX3wbVHIcHIKLx4kGeJChX7pdW38A+/dcb7qk/WIKdCW3/h6L6tIY/SJWVNbPaY2HE7gTCFeVSPGT/I0X9bXzSLFyeIq567108Muz5sZFBBe5PTwG/miFPWDcmjSNkjSWeuXu5ZKtBiYXBDQgF5+AbcvGDJb8XgVU0weLHj7dCpI0Q8QolVvsRo4IdWr0T6D1k7sXsfHoH2HS4iSL+mjQ11FdiBgt5StCmmVBUdpysOsLuVdJOTAnddqs1hYKokjyl6WQJXEvLAp163u5xJxiOkqjBVtecBLW/+DxiB1hFqydfVzgRIDkDkEFsziRnN6+TUEXQ+flHPl6ElHyzPILDjtWMVRYveYYS/56rDduSP0SINRXhHhikPSRDIcRmy/ny3w+YZN0hBjOhLXg5PRw+nzePX+eDhcLq1nkFsm83miRJmiAMrm5k6CnNhM5ojd2WCKHc3bcWaaMg3JabKYjBAJDHKwgzcbTzUKZSR98aGV90lPNMUIYPwVJJ1nthz8sHIIjm5SHZYlyWtJZi1rIlyIWnAAVbOOB4HHxwiqKyKJ3V45zADlUaIlRmkAPW1CPQtq/5KOcTyR786V/mlTej7g1d9q0U/GMdOhG97aE0Uorq2doYsTYUmYQ5bQnvw1RjsSCTfKJpNLgR+D6PWXVhoTa/gKePVWVLO5f/0pHQMReBEpLbwldWqNU4YomMGPzfL50daUhm8+uNFzgxQJ3Fcwe9BC869Zj/eUI7Ym4qD353ZI7nOSY7yfziY//8XwdO0dBL7iuUTUvB/cWfm2U4TLiIvYWWKRSC0gjnAYFahnnpu1MOV7HuHT5FHgs5nhZi12rCK2E9HNjxCwM+oudENMwQGjWuV1AuZsmqdIq/NgqqBCsVdETXuRpta/SM24RFVQhxhO9dltGqdblcO+URm7K8fhTe90PvLA3je0187Zi5aI86aElOH9xf+sX4I+Wag4BEz2EcQ0bDKBuFGrpm1E94Gdd0eYddd9VYFEijDbji4TOzSnWzrC5rlJRptIuT0ruGip/1C2cubnbELwLvmuH9yCDzSG0A8caNVhsbemduk5AMYh+zUUlLKiqtpwAPgCvLdGTU6vegLQnG1glGpN4shzODINSg0jX8+VSofIdM7iDCw5ehXcDdHLvWTc3AJNywx4oUNeRMQJ/uig60HXuumwKbyuDpVGn9SKaIWW+VXmUCX/eMMAbHxj1CNXv9QLYk3FgJeKy4+QoDufDOVRrIHPHlhzqimMQMTYwepLN2kUWptdXJRZnasuuyNIMok+mmYDKFEOC2ueFEb2QsP+J+CetNk5GYE/VWD9XO32V6NIcpqvvQasTI4xjJ6fgo12bYwBn9IdRVNpMdh1UsQ8E7n6/sFLEoFvuzedb050xvtIlNQhCTE7DYaZdajDXjuN5oH8Qwc0lQDl7Fnpt3eWkqyFzeLSRur2ArGBkykcOTdA7BMDroNVsJDQVf4bMYWF3V0bs6Dz97rEYe1ZAGpLtkwqZ5gl9khGHg15Dfx9Sldput5rG6rDQJ5AfpLPVC90zg8tq3KEh70PXrebKv0Cl2dSfnHMiP4hUBoNz+8nUk9vBkmcDjsI73g5qrmVDLASubaOPK9a+SpDYqw6/XWeAJ4eD92kq90F8d5i2jbEcsl9KHSKhCia+NVBBF56wZz6EgLeNhLvRfie+UdIE5p4ApD89XDL+DYrs9avfU4kOb3lQxV6XBHxtR/qd8lC6yZMNp4ukjWT8pMN2+esjgia7UHtB2cnow2J6HgfRNtpOIPdTaPqb2X4krrGozKUY/Wf7GQ7RsBnUV4zq5ZB8ltvrKIMQ8G3JCDx48EsYgE2Z5iqh8Ta+gustxFhBL+UWfWHYDAaBfy2eovPUL16rOnCCw7QKOD/kDAYRjOpXeJwW20xj8Pmn3PKTbLs4sSoN2mMzBPGh2keqfvt4uy+2d1mkmIuyu21x//bhP2BoACgR10nsvRFXKhEvKWS1my0GHDg54i8tLF2t/movKh0WicfrnLTR5d3JQySbcPve8ilaDdvrNqQxZowBZ3qjXVy6pvmmRnWdNM4HAs92nTjVm4TsYa45aebQANDlpuQyx2aNf4JFez8unu8WD43MOXooxJouCnz7J4Q0sHdPKvAgadinq7ePxkX8eFtNBRczm+AfcpbNJ4schRGUzhmK0ax4f1w/vOx2z7vdy8P68b2YjYGrhBxFTqzs8fqx7xA4gCjpTiugfQWlK4S69sn9QW868UCBBXv8J6whEl5VpqVUao+1gdNz5YleJYmqUMjJrxkN60S7i7zW55Dt0qbdpdkW6QvFWqF/b/fgCcC1yeRsUoiGq3Vwf4rCoF0qs/HN65P6JXu1jj7G36v7u5wIqhCKi+KVqqBxx34zwdozAeIDu0p2SediJANe053HRTDSd63p4AIZ/JdkFAnEmdU9mXJ3frxfrVb3j+dd6X2uyobaYPmGSiiGbz53l0GiiwtRUD+HfFjRQMomiIcKP3QHqSO7hhqH999iEUntRbxjrCt0CpWm+XdMvU+4itPTJUZ7XVaZyqr+k4SlJsEF5PolLdJ/UUINSR6TJ3NRnHJVXV8F/WUx+gdywN3Eg+JtTgKpNYxVrLtj+k/uwApBJA19tn2qWEBJ891jYduF/m3SLRdCRNlse3z/c3+/ej9uZ1lkw6ffZvJ/VV0LQw9IZ/0AAAAASUVORK5CYII="><link rel="canonical" href="http://example.com/2022/10/30/CS231n%204.1%EF%BC%9A%E6%9C%80%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%8A%EF%BC%89/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '最优化',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-10-30 19:50:28'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://marketplace.canva.cn/EAEsSrIavXA/2/0/1600w/canva-lqyXLROcj-k.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/%20category/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">炸牛奶超级甜</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/%20category/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">最优化</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-10-29T16:00:00.000Z" title="发表于 2022-10-30 00:00:00">2022-10-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-10-30T11:50:28.604Z" title="更新于 2022-10-30 19:50:28">2022-10-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="最优化"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><p>内容列表：</p>
<ul>
<li><p>简介</p></li>
<li><p>损失函数可视化</p>
<p>最优化</p>
<ul>
<li>策略#1：随机搜索</li>
<li>策略#2：随机局部搜索</li>
<li>策略#3：跟随梯度</li>
</ul></li>
<li><p>梯度计算</p>
<ul>
<li>使用有限差值进行数值计算</li>
<li>微分计算梯度</li>
</ul></li>
<li><p>梯度下降</p></li>
<li><p>小结</p></li>
</ul>
<h1 id="最优化">最优化</h1>
<p>在上一节中，我们介绍了图像分类任务中的两个关键部分：</p>
<ol type="1">
<li>基于参数的<strong>评分函数。</strong>该函数将原始图像像素映射为分类评分值（例如：一个线性函数）。</li>
<li><strong>损失函数</strong>。该函数能够根据分类评分和训练集图像数据实际分类的一致性，衡量某个具体参数集的质量好坏。损失函数有多种版本和不同的实现方式（例如：Softmax或SVM）。</li>
</ol>
<p>上节中，线性函数的形式是<span class="math inline">\(f(x_i,
W)=Wx_i\)</span>，而SVM实现的公式是：</p>
<p><span class="math inline">\(\frac{1}{N}\sum_i\sum_{j \neq
y_i}[max(0,f(x_i;W)_j-f(x_i;W)_{y_i}+1)]+\alpha R(W)\)</span></p>
<p>对于图像数据<span
class="math inline">\(x_i\)</span>，如果基于参数集<span
class="math inline">\(W\)</span>做出的分类预测与真实情况比较一致，那么计算出来的损失值<span
class="math inline">\(L\)</span>就很低。现在介绍第三个，也是最后一个关键部分：<strong>最优化Optimization</strong>。最优化是寻找能使得损失函数值最小化的参数<span
class="math inline">\(W\)</span>的过程。</p>
<p><strong>铺垫</strong>：一旦理解了这三个部分是如何相互运作的，我们将会回到第一个部分（基于参数的函数映射），然后将其拓展为一个远比线性函数复杂的函数：首先是神经网络，然后是卷积神经网络。而损失函数和最优化过程这两个部分将会保持相对稳定。</p>
<h2 id="损失函数可视化">损失函数可视化</h2>
<p>本课中讨论的损失函数一般都是定义在高维度的空间中（比如，在CIFAR-10中一个线性分类器的权重矩阵大小是[10x3073]，就有30730个参数），这样要将其可视化就很困难。然而办法还是有的，在1个维度或者2个维度的方向上对高维空间进行切片，就能得到一些直观感受。例如，随机生成一个权重矩阵W，该矩阵就与高维空间中的一个点对应。然后沿着某个维度方向前进的同时记录损失函数值的变化。换句话说，就是生成一个随机的方向W并且沿着此方向计算损失值，计算方法是根据不同的<span
class="math inline">\(a\)</span>值来计算<span
class="math inline">\(L(W+aW_1)\)</span>。这个过程将生成一个图表，其x轴是<span
class="math inline">\(a\)</span>值，y轴是损失函数值。同样的方法还可以用在两个维度上，通过改变<span
class="math inline">\(a,b\)</span>来计算损失值<span
class="math inline">\(L(W+aW_1+bW_2)\)</span>，从而给出二维的图像。在图像中，<span
class="math inline">\(a,b\)</span>可以分别用x和y轴表示，而损失函数的值可以用颜色变化表示：</p>
<p>————————————————————————————————————————</p>
<figure>
<img
src="https://caojun-2014.oss-cn-beijing.aliyuncs.com/image-20221030183544919.png"
alt="image-20221030183544919" />
<figcaption aria-hidden="true">image-20221030183544919</figcaption>
</figure>
<p>一个无正则化的多类SVM的损失函数的图示。左边和中间只有一个样本数据，右边是CIFAR-10中的100个数据。<strong>左</strong>：a值变化在某个维度方向上对应的的损失值变化。<strong>中和右</strong>：两个维度方向上的损失值切片图，蓝色部分是低损失值区域，红色部分是高损失值区域。注意损失函数的分段线性结构。多个样本的损失值是总体的平均值，所以右边的碗状结构是很多的分段线性结构的平均（比如中间这个就是其中之一）。</p>
<p>—————————————————————————————————————————</p>
<p>​
我们可以通过数学公式来解释损失函数的分段线性结构。对于一个单独的数据，有损失函数的计算公式如下：</p>
<p><span class="math inline">\(L_i=\sum_{j!=y_i}[max(0,w_j^T
x_i-w_{y_i}^T +1)]\)</span></p>
<p>​ 通过公式可见，每个样本的数据损失值是以<span
class="math inline">\(W\)</span>为参数的线性函数的总和（零阈值来源于<span
class="math inline">\(max(0,-)\)</span>函数）。<span
class="math inline">\(W\)</span>的每一行（即<span
class="math inline">\(W_j\)</span>），有时候它前面是一个正号（比如当它对应错误分类的时候），有时候它前面是一个负号（比如当它是是正确分类的时候）。为进一步阐明，假设有一个简单的数据集，其中包含有3个只有1个维度的点，数据集数据点有3个类别。那么完整的无正则化SVM的损失值计算如下：</p>
<p><span
class="math inline">\(L_0=max(0,w^T_1x_0-w^T_0x_0+1)+max(0,w^T_2x_0-w^T_0x_0+1)\)</span><br />
<span
class="math inline">\(L_1=max(0,w^T_0x_1-w^T_1x_1+1)+max(0,w^T_2x_1-w^T_1x_1+1)\)</span><br />
<span
class="math inline">\(L_2=max(0,w^T_0x_2-w^T_2x_2+1)+max(0,w^T_1x_2-w^T_2x_2+1)\)</span></p>
<p><span class="math inline">\(L=(L_0+L_1+L_2)/3\)</span></p>
<p>​ 因为这些例子都是一维的，所以数据<span
class="math inline">\(x_i\)</span>和权重<span
class="math inline">\(w_j\)</span>都是数字。观察<span
class="math inline">\(w_0\)</span>，可以看到上面的式子中一些项是<span
class="math inline">\(W_0\)</span>的线性函数，且每一项都会与0比较，取两者的最大值。可作图如下：——————————————————————————————————————</p>
<figure>
<img
src="https://caojun-2014.oss-cn-beijing.aliyuncs.com/image-20221030183719966.png"
alt="image-20221030183719966" />
<figcaption aria-hidden="true">image-20221030183719966</figcaption>
</figure>
<p>从一个维度方向上对数据损失值的展示。==x轴方向就是一个W权重，y轴就是损失值==。数据损失是多个部分组合而成。其中每个部分要么是某个权重的独立部分，要么是该权重的线性函数与0阈值的比较。完整的SVM数据损失就是这个形状的30730维版本。</p>
<p>这里可以理解成对<span class="math inline">\(L\)</span>中的<span
class="math inline">\(w_0,w_1,w_2\)</span>求导之后的系数是左边图折线的斜率，所以蓝色是<span
class="math inline">\(w_0\)</span>代表的线段，斜率为负表示对<span
class="math inline">\(w_0\)</span>求导之后是<span
class="math inline">\(-x_0\)</span>所以是减少的，其他的两条<span
class="math inline">\(L_2,L_3\)</span>对<span
class="math inline">\(w_0\)</span>求导之后是正数，所以是增加的。</p>
<p>——————————————————————————————————————</p>
<p>需要多说一句的是，你可能根据SVM的损失函数的碗状外观猜出它是一个[凸函数__][31]。关于如何高效地最小化凸函数的论文有很多，你也可以学习斯坦福大学关于（[凸函数最优化__][32]）的课程。但是一旦我们将<span
class="math inline">\(f\)</span>函数扩展到神经网络，目标函数就就不再是凸函数了，图像也不会像上面那样是个碗状，而是凹凸不平的复杂地形形状。</p>
<p>_不可导的损失函数。_作为一个技术笔记，你要注意到：由于max操作，损失函数中存在一些_不可导点（kinks），_这些点使得损失函数不可微，因为在这些不可导点，梯度是没有定义的。但是[次梯度（subgradient）__][34]依然存在且常常被使用。在本课中，我们将交换使用_次梯度_和_梯度_两个术语。</p>
<h2 id="最优化-optimization">最优化 Optimization</h2>
<p>重申一下：损失函数可以量化某个具体权重集<strong>W</strong>的质量。而最优化的目标就是找到能够最小化损失函数值的<strong>W</strong>
。我们现在就朝着这个目标前进，实现一个能够最优化损失函数的方法。对于有一些经验的同学，这节课看起来有点奇怪，因为使用的例子（SVM
损失函数）是一个凸函数问题。但是要记得，最终的目标是不仅仅对凸函数做最优化，而是能够最优化一个神经网络，而对于神经网络是不能简单的使用凸函数的最优化技巧的。</p>
<h3
id="策略1一个差劲的初始方案随机搜索">策略1：一个差劲的初始方案：随机搜索</h3>
<p>既然确认参数集<strong>W</strong>的好坏蛮简单的，那第一个想到的（差劲）方法，就是可以随机尝试很多不同的权重，然后看其中哪个最好。过程如下：</p>
<p>​<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment"># 假设X_train的每一列都是一个数据样本（比如3073 x 50000）</span></span><br><span class="line">    <span class="comment"># 假设Y_train是数据样本的类别标签（比如一个长50000的一维数组）</span></span><br><span class="line">    <span class="comment"># 假设函数L对损失函数进行评价</span></span><br><span class="line">    </span><br><span class="line">    bestloss = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>) <span class="comment"># Python assigns the highest possible float value</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> xrange(<span class="number">1000</span>):</span><br><span class="line">      W = np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.0001</span> <span class="comment"># generate random parameters</span></span><br><span class="line">      loss = L(X_train, Y_train, W) <span class="comment"># get the loss over the entire training set</span></span><br><span class="line">      <span class="keyword">if</span> loss &amp;lt; bestloss: <span class="comment"># keep track of the best solution</span></span><br><span class="line">        bestloss = loss</span><br><span class="line">        bestW = W</span><br><span class="line">      <span class="built_in">print</span> <span class="string">&#x27;in attempt %d the loss was %f, best %f&#x27;</span> % (num, loss, bestloss)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出:</span></span><br><span class="line">    <span class="comment"># in attempt 0 the loss was 9.401632, best 9.401632</span></span><br><span class="line">    <span class="comment"># in attempt 1 the loss was 8.959668, best 8.959668</span></span><br><span class="line">    <span class="comment"># in attempt 2 the loss was 9.044034, best 8.959668</span></span><br><span class="line">    <span class="comment"># in attempt 3 the loss was 9.278948, best 8.959668</span></span><br><span class="line">    <span class="comment"># in attempt 4 the loss was 8.857370, best 8.857370</span></span><br><span class="line">    <span class="comment"># in attempt 5 the loss was 8.943151, best 8.857370</span></span><br><span class="line">    <span class="comment"># in attempt 6 the loss was 8.605604, best 8.605604</span></span><br><span class="line">    <span class="comment"># ... (trunctated: continues for 1000 lines)</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment">#在上面的代码中，我们尝试了若干随机生成的权重矩阵**W**，其中某些的损失值较小，而另一些的损失值大些。我们可以把这次随机搜索中找到的最好的权重**W**取出，然后去跑测试集：  </span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 假设X_test尺寸是[3073 x 10000], Y_test尺寸是[10000 x 1]</span></span><br><span class="line">    scores = Wbest.dot(Xte_cols) <span class="comment"># 10 x 10000, the class scores for all test examples</span></span><br><span class="line">    <span class="comment"># 找到在每列中评分值最大的索引（即预测的分类）</span></span><br><span class="line">    Yte_predict = np.argmax(scores, axis = <span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 以及计算准确率</span></span><br><span class="line">    np.mean(Yte_predict == Yte)</span><br><span class="line">    <span class="comment"># 返回 0.1555</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>验证集上表现最好的权重<strong>W</strong>跑测试集的准确率是<strong>15.5%，</strong>而完全随机猜的准确率是10%，如此看来，这个准确率对于这样一个不经过大脑的策略来说，还算不错嘛！</p>
<p><strong>核心思路：迭代优化</strong>。当然，我们肯定能做得更好些。核心思路是：虽然找到最优的权重<strong>W</strong>非常困难，甚至是不可能的（尤其当<strong>W</strong>中存的是整个神经网络的权重的时候），但如果问题转化为：对一个权重矩阵集<strong>W</strong>取优，使其损失值稍微减少。那么问题的难度就大大降低了。换句话说，我们的方法从一个随机的<strong>W</strong>开始，然后对其迭代取优，每次都让它的损失值变得更小一点。</p>
<p>&gt;
我们的策略是从随机权重开始，然后迭代取优，从而获得更低的损失值。</p>
<p><strong>蒙眼徒步者的比喻</strong>：一个助于理解的比喻是把你自己想象成一个蒙着眼睛的徒步者，正走在山地地形上，目标是要慢慢走到山底。在CIFAR-10的例子中，这山是30730维的（因为<strong>W</strong>是3073x10）。我们在山上踩的每一点都对应一个的损失值，该损失值可以看做该点的海拔高度。</p>
<h3 id="策略2随机本地搜索">策略2：随机本地搜索</h3>
<p>第一个策略可以看做是每走一步都尝试几个随机方向，如果某个方向是向山下的，就向该方向走一步。这次我们从一个随机W开始，然后生成一个随机的扰动<span
class="math inline">\(\delta W\)</span> ，只有当<span
class="math inline">\(W+\delta
W\)</span>的损失值变低，我们才会更新。</p>
<p><strong>简单来说就是：</strong>==可以想象一个人在山坡上，然后随机的四面八方走，如果走的位置比当前位置更低就确定这个方向。==</p>
<p>这个过程的具体代码如下：</p>
<p>​<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.001</span> <span class="comment"># 生成随机初始W</span></span><br><span class="line">bestloss = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1000</span>):</span><br><span class="line">  step_size = <span class="number">0.0001</span></span><br><span class="line">  Wtry = W + np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * step_size</span><br><span class="line">  loss = L(Xtr_cols, Ytr, Wtry)</span><br><span class="line">  <span class="keyword">if</span> loss &amp;lt; bestloss:</span><br><span class="line">    W = Wtry</span><br><span class="line">    bestloss = loss</span><br><span class="line">  <span class="built_in">print</span> <span class="string">&#x27;iter %d loss is %f&#x27;</span> % (i, bestloss)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<p>使用同样的数据（1000），这个方法可以得到<strong>21.4%</strong>的分类准确率。这个比策略一好，但是依然过于浪费计算资源。</p>
<h3 id="策略3沿着梯度方向">策略3：沿着梯度方向</h3>
<p>也就是梯度的方向</p>
<p>前两个策略中，我们是尝试在权重空间中找到一个方向，沿着该方向能降低损失函数的损失值。其实不需要随机寻找方向，因为可以直接计算出最好的方向，这就是从数学上计算出最陡峭的方向。这个方向就是损失函数的<strong>梯度（gradient）</strong>。在蒙眼徒步者的比喻中，这个方法就好比是感受我们脚下山体的倾斜程度，然后向着最陡峭的下降方向下山。</p>
<p>在一维函数中，斜率是函数在某一点的瞬时变化率。梯度是函数的斜率的一般化表达，它不是一个值，而是一个向量。在输入空间中，梯度是各个维度的斜率组成的向量（或者称为导数<strong>derivatives</strong>）。对一维函数的求导公式如下：</p>
<p><span class="math inline">\(\frac{df(x)}{dx}=lim_{h \to
0}\frac{f(x+h)-f(x)}{h}\)</span></p>
<p>当函数有多个参数的时候，我们称导数为偏导数。而梯度就是在每个维度上偏导数所形成的向量。</p>
<h2 id="梯度计算">梯度计算</h2>
<p>计算梯度有两种方法：一个是缓慢的近似方法（<strong>数值梯度法</strong>），但实现相对简单。另一个方法（<strong>分析梯度法</strong>）计算迅速，结果精确，但是实现时容易出错，且需要使用微分。现在对两种方法进行介绍：</p>
<h3 id="利用数值法计算梯度">利用数值法计算梯度</h3>
<p>上节中的公式已经给出数值计算梯度的方法。下面代码是一个输入为函数<strong>f</strong>和向量<strong>x，</strong>计算<strong>f</strong>的梯度的通用函数，它返回函数<strong>f</strong>在点<strong>x处</strong>的梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">def</span> <span class="title function_">eval_numerical_gradient</span>(<span class="params">f, x</span>):</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">a naive implementation of numerical gradient of f at x</span></span><br><span class="line"><span class="string">- f should be a function that takes a single argument</span></span><br><span class="line"><span class="string">- x is the point (numpy array) to evaluate the gradient at</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"> <span class="string">&quot;&quot;&quot;  </span></span><br><span class="line"><span class="string">    一个f在x处的数值梯度法的简单实现</span></span><br><span class="line"><span class="string">    - f是只有一个参数的函数</span></span><br><span class="line"><span class="string">    - x是计算梯度的点</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span> </span><br><span class="line">fx = f(x) <span class="comment"># evaluate function value at original point # 在原点计算当前点</span></span><br><span class="line">grad = np.zeros(x.shape)</span><br><span class="line">h = <span class="number">0.00001</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># iterate over all indexes in x# 对x中所有的索引进行迭代</span></span><br><span class="line">it = np.nditer(x, flags=[<span class="string">&#x27;multi_index&#x27;</span>], op_flags=[<span class="string">&#x27;readwrite&#x27;</span>])</span><br><span class="line"><span class="keyword">while</span> <span class="keyword">not</span> it.finished:</span><br><span class="line"></span><br><span class="line">  <span class="comment"># evaluate function at x+h # 计算x+h处的函数值</span></span><br><span class="line">  ix = it.multi_index</span><br><span class="line">  old_value = x[ix]</span><br><span class="line">  x[ix] = old_value + h <span class="comment"># increment by h </span></span><br><span class="line">  fxh = f(x) <span class="comment"># evalute f(x + h)计算f(x + h)</span></span><br><span class="line">  x[ix] = old_value <span class="comment"># restore to previous value (very important!)存到前一个值中 (非常重要)</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># compute the partial derivative</span></span><br><span class="line">  grad[ix] = (fxh - fx) / h <span class="comment"># the slope# 坡度</span></span><br><span class="line">  it.iternext() <span class="comment"># step to next dimension到下个维度</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure>
<p>根据上面的梯度公式，代码对所有维度进行迭代，在每个维度上产生一个很小的变化h，通过观察函数值变化，计算函数在该维度上的偏导数。最后，所有的梯度存储在变量<strong>grad</strong>中。</p>
<h4 id="实践">实践：</h4>
<p>注意在数学公式中，<strong>h</strong>的取值是趋近于0的，然而在实际中，==用一个很小的数值（比如例子中的1e-5）==就足够了。在不产生数值计算出错的理想前提下，你会使用尽可能小的h。还有，实际中用**中心差值公式<span
class="math inline">\([f(x+h)-f(x-h)]/2h]\)</span>效果较好。细节可查看[wiki__][12]。</p>
<p>可以使用上面这个公式来计算任意函数在任意点上的梯度。下面计算权重空间中的某些随机点上，CIFAR-10损失函数的梯度：</p>
<p>​</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 要使用上面的代码我们需要一个只有一个参数的函数</span></span><br><span class="line"><span class="comment"># (在这里参数就是权重)所以也包含了X_train和Y_train</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">CIFAR10_loss_fun</span>(<span class="params">W</span>):</span><br><span class="line">  <span class="keyword">return</span> L(X_train, Y_train, W)</span><br><span class="line"></span><br><span class="line">W = np.random.rand(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.001</span> <span class="comment"># 随机权重向量</span></span><br><span class="line">df = eval_numerical_gradient(CIFAR10_loss_fun, W) <span class="comment"># 得到梯度</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>梯度告诉我们损失函数在每个维度上的斜率，以此来进行更新：</p>
<p>​</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">loss_original = CIFAR10_loss_fun(W) <span class="comment"># 初始损失值</span></span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;original loss: %f&#x27;</span> % (loss_original, )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看不同步长的效果</span></span><br><span class="line"><span class="keyword">for</span> step_size_log <span class="keyword">in</span> [-<span class="number">10</span>, -<span class="number">9</span>, -<span class="number">8</span>, -<span class="number">7</span>, -<span class="number">6</span>, -<span class="number">5</span>,-<span class="number">4</span>,-<span class="number">3</span>,-<span class="number">2</span>,-<span class="number">1</span>]:</span><br><span class="line">  step_size = <span class="number">10</span> ** step_size_log</span><br><span class="line">  W_new = W - step_size * df <span class="comment"># 权重空间中的新位置</span></span><br><span class="line">  loss_new = CIFAR10_loss_fun(W_new)</span><br><span class="line">  <span class="built_in">print</span> <span class="string">&#x27;for step size %f new loss: %f&#x27;</span> % (step_size, loss_new)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># original loss: 2.200718</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-10 new loss: 2.200652</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-09 new loss: 2.200057</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-08 new loss: 2.194116</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-07 new loss: 2.135493</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-06 new loss: 1.647802</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-05 new loss: 2.844355</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-04 new loss: 25.558142</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-03 new loss: 254.086573</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-02 new loss: 2539.370888</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-01 new loss: 25392.214036</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="在梯度负方向上更新">在梯度负方向上更新</h4>
<p>在上面的代码中，为了计算<strong>W_new</strong>，要注意我们是向着梯度<strong>df</strong>的负方向去更新，这是因为我们希望损失函数值是降低而不是升高。</p>
<h4 id="步长的影响">步长的影响：</h4>
<p>梯度指明了函数在哪个方向是变化率最大的，但是没有指明在这个方向上应该走多远。在后续的课程中可以看到，选择步长（也叫作_学习率_）将会是神经网络训练中最重要（也是最头痛）的超参数设定之一。还是用蒙眼徒步者下山的比喻，这就好比我们可以感觉到脚朝向的不同方向上，地形的倾斜程度不同。但是该跨出多长的步长呢？不确定。如果谨慎地小步走，情况可能比较稳定但是进展较慢（这就是步长较小的情况）。相反，如果想尽快下山，那就大步走吧，但结果也不一定尽如人意。在上面的代码中就能看见反例，在某些点如果步长过大，反而可能越过最低点导致更高的损失值。</p>
<p>————————————————————————————————————————</p>
<figure>
<img
src="https://caojun-2014.oss-cn-beijing.aliyuncs.com/image-20221030171346808.png"
alt="image-20221030171346808" />
<figcaption aria-hidden="true">image-20221030171346808</figcaption>
</figure>
<p>将步长效果视觉化的图例。从某个具体的点W开始计算梯度（白箭头方向是负梯度方向），梯度告诉了我们损失函数下降最陡峭的方向。小步长下降稳定但进度慢，大步长进展快但是风险更大。采取大步长可能导致错过最优点，让损失值上升。步长（后面会称其为<strong>学习率</strong>）将会是我们在调参中最重要的超参数之一。</p>
<p>————————————————————————————————————————</p>
<p><strong>效率问题</strong>：你可能已经注意到，计算数值梯度的复杂性和参数的量线性相关。在本例中有30730个参数，==所以损失函数每走一步就需要计算30731次损失函数的梯度==。现代神经网络很容易就有上千万的参数，因此这个问题只会越发严峻。显然这个策略不适合大规模数据，我们需要更好的策略。</p>
<h3 id="微分分析计算梯度">微分分析计算梯度</h3>
<p>使用有限差值近似计算梯度比较简单，但缺点在于终究只是近似（因为我们对于_h_值是选取了一个很小的数值，但真正的梯度定义中_h_趋向0的极限），且耗费计算资源太多。第二个梯度计算方法是利用微分来分析，能得到计算梯度的公式（不是近似），用公式计算梯度速度很快，==唯一不好的就是实现的时候容易出错==。为了解决这个问题，在实际操作时常常将分析梯度法的结果和数值梯度法的结果作比较，以此来检查其实现的正确性，这个步骤叫做<strong>梯度检查</strong>。</p>
<p>用SVM的损失函数在某个数据点上的计算来举例：</p>
<p><span class="math inline">\(L_i=\sum_{j \neq
y_i}[max(0,w^T_jx_i-w^T_{y_i}x_i+\Delta)]\)</span></p>
<p>可以对函数进行微分。比如，对<span
class="math inline">\(w_{y_i}\)</span>进行微分得到：</p>
<p><span class="math inline">\(\nabla {w_{y_i}}L_i=-(\sum_{j \neq
y_i}1(w^T_jx_i-w^T_{y_i}x_i+\Delta&gt;0))x_i\)</span></p>
<p>其中1是一个示性函数，如果括号中的条件为真，那么函数值为1，如果为假，则函数值为0。虽然上述公式看起来复杂，但在代码实现的时候比较简单：只需要计算没有满足边界值的分类的数量（因此对损失函数产生了贡献），然后乘以<span
class="math inline">\(x_i\)</span>就是梯度了。注意，这个梯度只是对应正确分类的W的行向量的梯度，那些<span
class="math inline">\(j \neq y_i\)</span>行的梯度是：</p>
<p><span
class="math inline">\(\nabla_{w_j}L_i=1(w^T_jx_i-w^T_{y_i}x_i+\Delta
&gt;0)x_i\)</span></p>
<p>一旦将梯度的公式微分出来，代码实现公式并用于梯度更新就比较顺畅了。</p>
<figure>
<img
src="https://caojun-2014.oss-cn-beijing.aliyuncs.com/image-20221030181051471.png"
alt="image-20221030181051471" />
<figcaption aria-hidden="true">image-20221030181051471</figcaption>
</figure>
<p><span
class="math display">\[L=max(0,S_1-S_0+1)+max(0,S_2-S_0+1\]</span></p>
<p><span
class="math inline">\(L=max(0,W_1X-W_0X+1)+max(0,W_2X-W_0X+1)\)</span></p>
<p><span class="math inline">\(\frac{dL}{dW_0}=-X\sum_{j \neq y_i}
(W_jX-W_{y_i}X+1&gt;0)\)</span></p>
<p><span class="math inline">\(\frac{dL}{dW_1}=X
(W_1X-W_{0}X+1&gt;0)\)</span></p>
<h2 id="梯度下降">梯度下降</h2>
<p>现在可以计算损失函数的梯度了，程序重复地计算梯度然后对参数进行更新，这一过程称为_梯度下降_，他的<strong>普通</strong>版本是这样的：</p>
<p>​</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通的梯度下降</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">  weights_grad = evaluate_gradient(loss_fun, data, weights)</span><br><span class="line">  weights += - step_size * weights_grad <span class="comment"># 进行梯度更新</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这个简单的循环在所有的神经网络核心库中都有。虽然也有其他实现最优化的方法（比如LBFGS），但是到目前为止，==梯度下降是对神经网络的损失函数最优化中最常用的方法==。课程中，我们会在它的循环细节增加一些新的东西（比如更新的具体公式），但是核心思想不变，那就是我们一直跟着梯度走，直到结果不再变化。</p>
<p><strong>小批量数据梯度下降（</strong>Mini-batch gradient
descent）<strong>：在大规模的应用中（比如ILSVRC挑战赛），训练数据可以达到百万级量级。如果像这样计算整个训练集，来获得仅仅一个参数的更新就太浪费了。一个常用的方法是计算训练集中的</strong>小批量（batches）数据。例如，在目前最高水平的卷积神经网络中，一个典型的小批量包含256个例子，而整个训练集是多少呢？一百二十万个。也就是说：<span
class="math inline">\(L=(L_0+L_1+L_2+....)/N\)</span>实在太多了</p>
<p>这个小批量数据就用来实现一个参数更新：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通的小批量数据梯度下降</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">  data_batch = sample_training_data(data, <span class="number">256</span>) <span class="comment"># 256个数据</span></span><br><span class="line">  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)</span><br><span class="line">  weights += - step_size * weights_grad <span class="comment"># 参数更新</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这个方法之所以效果不错，是因为训练集中的数据都是相关的。要理解这一点，可以想象一个极端情况：在ILSVRC中的120万个图像是1000张不同图片的复制（每个类别1张图片，每张图片有1200张复制）。==那么显然计算这1200张复制图像的梯度就应该是一样的==。对比120万张图片的数据损失的均值与只计算1000张的子集的数据损失均值时，结果应该是一样的。实际情况中，数据集肯定不会包含重复图像，那么小批量数据的梯度就是对整个数据集梯度的一个近似。因此，==在实践中通过计算小批量数据的梯度可以实现更快速地收敛，并以此来进行更频繁的参数更新==。</p>
<p>小批量数据策略有个极端情况，那就是每个批量中只有1个数据样本，这种策略被称为<strong>随机梯度下降（Stochastic
Gradient Descent
简称SGD）</strong>，有时候也被称为在线梯度下降。这种策略在实际情况中相对少见，因为向量化操作的代码一次计算100个数据
比100次计算1个数据要高效很多。即使SGD在技术上是指每次使用1个数据来计算梯度，你还是会听到人们使用SGD来指代小批量数据梯度下降（或者用MGD来指代小批量数据梯度下降，而BGD来指代则相对少见）。小批量数据的大小是一个超参数，但是一般并不需要通过交叉验证来调参。它一般由存储器的限制来决定的，或者干脆设置为同样大小，比如32，64，128等。之所以使用2的指数，是因为在实际中许多向量化操作实现的时候，如果输入数据量是2的倍数，那么运算更快。</p>
<h2 id="小结">小结</h2>
<p>————————————————————————————————————————</p>
<figure>
<img
src="https://caojun-2014.oss-cn-beijing.aliyuncs.com/image-20221030183033809.png"
alt="image-20221030183033809" />
<figcaption aria-hidden="true">image-20221030183033809</figcaption>
</figure>
<p>信息流的总结图例。数据集中的(x,y)是给定的。权重从一个随机数字开始，且可以改变。在前向传播时，评分函数计算出类别的分类评分并存储在向量<strong>f</strong>中。损失函数包含两个部分：数据损失和正则化损失。其中，数据损失计算的是分类评分f和实际标签y之间的差异，正则化损失只是一个关于权重的函数。在梯度下降过程中，我们计算权重的梯度（如果愿意的话，也可以计算数据上的梯度），然后使用它们来实现参数的更新。</p>
<p>—————————————————————————————————————————</p>
<p>在本节课中：</p>
<ul>
<li><p>将损失函数比作了一个<strong>高维度的最优化地形</strong>，并尝试到达它的最底部。最优化的工作过程可以看做一个蒙着眼睛的徒步者希望摸索着走到山的底部。在例子中，可见SVM的损失函数是分段线性的，并且是碗状的。</p></li>
<li><p>提出了迭代优化的思想，从一个随机的权重开始，然后一步步地让损失值变小，直到最小。</p></li>
<li><p>函数的<strong>梯度</strong>给出了该函数最陡峭的上升方向。介绍了利用有限的差值来近似计算梯度的方法，该方法实现简单但是效率较低（有限差值就是_h_，用来计算数值梯度）。</p></li>
<li><p>参数更新需要有技巧地设置<strong>步长</strong>。也叫学习率。如果步长太小，进度稳定但是缓慢，如果步长太大，进度快但是可能有风险。</p></li>
<li><p>讨论权衡了数值梯度法和分析梯度法。数值梯度法计算简单，但结果只是近似且耗费计算资源。分析梯度法计算准确迅速但是实现容易出错，而且需要对梯度公式进行推导的数学基本功。因此，在实际中使用分析梯度法，然后使用<strong>梯度检查</strong>来检查其实现正确与否，其本质就是将分析梯度法的结果与数值梯度法的计算结果对比。</p></li>
<li><p>介绍了<strong>梯度下降</strong>算法，它在循环中迭代地计算梯度并更新参数。</p></li>
</ul>
<p><strong>预告</strong>：这节课的核心内容是：理解并能计算损失函数关于权重的梯度，是设计、训练和理解神经网络的核心能力。下节中，将介绍如何使用链式法则来高效地计算梯度，也就是通常所说的<strong>反向传播（backpropagation）机制</strong>。该机制能够对包含卷积神经网络在内的几乎所有类型的神经网络的损失函数进行高效的最优化。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Jun Cao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2022/10/30/CS231n%204.1%EF%BC%9A%E6%9C%80%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%8A%EF%BC%89/">http://example.com/2022/10/30/CS231n%204.1%EF%BC%9A%E6%9C%80%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%8A%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">炸牛奶超级甜</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%80%E4%BC%98%E5%8C%96/">最优化</a><a class="post-meta__tags" href="/tags/%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5/">优化策略</a><a class="post-meta__tags" href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">梯度下降</a></div><div class="post_share"><div class="social-share" data-image="https://img-blog.csdnimg.cn/20181222203042133" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2022/10/29/svm%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%99%A8/"><img class="next-cover" src="https://i2.wp.com/dataaspirant.com/wp-content/uploads/2017/01/Support-vector-machine-svm.jpg?w=1024&amp;ssl=1" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">svm图像分类器</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://marketplace.canva.cn/EAEsSrIavXA/2/0/1600w/canva-lqyXLROcj-k.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Jun Cao</div><div class="author-info__description">简单记录学习生活</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%80%E4%BC%98%E5%8C%96"><span class="toc-number">1.</span> <span class="toc-text">最优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">1.1.</span> <span class="toc-text">损失函数可视化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%80%E4%BC%98%E5%8C%96-optimization"><span class="toc-number">1.2.</span> <span class="toc-text">最优化 Optimization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A51%E4%B8%80%E4%B8%AA%E5%B7%AE%E5%8A%B2%E7%9A%84%E5%88%9D%E5%A7%8B%E6%96%B9%E6%A1%88%E9%9A%8F%E6%9C%BA%E6%90%9C%E7%B4%A2"><span class="toc-number">1.2.1.</span> <span class="toc-text">策略1：一个差劲的初始方案：随机搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A52%E9%9A%8F%E6%9C%BA%E6%9C%AC%E5%9C%B0%E6%90%9C%E7%B4%A2"><span class="toc-number">1.2.2.</span> <span class="toc-text">策略2：随机本地搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A53%E6%B2%BF%E7%9D%80%E6%A2%AF%E5%BA%A6%E6%96%B9%E5%90%91"><span class="toc-number">1.2.3.</span> <span class="toc-text">策略3：沿着梯度方向</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">1.3.</span> <span class="toc-text">梯度计算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A9%E7%94%A8%E6%95%B0%E5%80%BC%E6%B3%95%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6"><span class="toc-number">1.3.1.</span> <span class="toc-text">利用数值法计算梯度</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E8%B7%B5"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">实践：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%A8%E6%A2%AF%E5%BA%A6%E8%B4%9F%E6%96%B9%E5%90%91%E4%B8%8A%E6%9B%B4%E6%96%B0"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">在梯度负方向上更新</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A5%E9%95%BF%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">1.3.1.3.</span> <span class="toc-text">步长的影响：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AE%E5%88%86%E5%88%86%E6%9E%90%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6"><span class="toc-number">1.3.2.</span> <span class="toc-text">微分分析计算梯度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.4.</span> <span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">1.5.</span> <span class="toc-text">小结</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/10/30/CS231n%204.1%EF%BC%9A%E6%9C%80%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%8A%EF%BC%89/" title="最优化"><img src="https://img-blog.csdnimg.cn/20181222203042133" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="最优化"/></a><div class="content"><a class="title" href="/2022/10/30/CS231n%204.1%EF%BC%9A%E6%9C%80%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%8A%EF%BC%89/" title="最优化">最优化</a><time datetime="2022-10-29T16:00:00.000Z" title="发表于 2022-10-30 00:00:00">2022-10-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/10/29/svm%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%99%A8/" title="svm图像分类器"><img src="https://i2.wp.com/dataaspirant.com/wp-content/uploads/2017/01/Support-vector-machine-svm.jpg?w=1024&amp;ssl=1" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="svm图像分类器"/></a><div class="content"><a class="title" href="/2022/10/29/svm%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%99%A8/" title="svm图像分类器">svm图像分类器</a><time datetime="2022-10-29T07:17:45.000Z" title="发表于 2022-10-29 15:17:45">2022-10-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/10/29/HDFS%E6%8A%80%E6%9C%AF/" title="HDFS技术"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HDFS技术"/></a><div class="content"><a class="title" href="/2022/10/29/HDFS%E6%8A%80%E6%9C%AF/" title="HDFS技术">HDFS技术</a><time datetime="2022-10-28T18:19:50.000Z" title="发表于 2022-10-29 02:19:50">2022-10-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/10/29/HDFS%E7%9A%84shell%E5%91%BD%E4%BB%A4/" title="HDFS的shell命令"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HDFS的shell命令"/></a><div class="content"><a class="title" href="/2022/10/29/HDFS%E7%9A%84shell%E5%91%BD%E4%BB%A4/" title="HDFS的shell命令">HDFS的shell命令</a><time datetime="2022-10-28T17:47:23.621Z" title="发表于 2022-10-29 01:47:23">2022-10-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/10/29/HDFS%E7%9A%84JavaAPI%E6%93%8D%E4%BD%9C/" title="HDFS的JavaAPI操作"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HDFS的JavaAPI操作"/></a><div class="content"><a class="title" href="/2022/10/29/HDFS%E7%9A%84JavaAPI%E6%93%8D%E4%BD%9C/" title="HDFS的JavaAPI操作">HDFS的JavaAPI操作</a><time datetime="2022-10-28T17:47:23.614Z" title="发表于 2022-10-29 01:47:23">2022-10-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Jun Cao</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a target="_blank" rel="noopener" href="https://butterfly.js.org/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>